{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroExpressionDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.label_encoding = {'anger': 0, 'disgust': 1, 'happiness': 2}  # Update as needed\n",
    "        \n",
    "        for category in sorted(os.listdir(directory)):\n",
    "            class_dir = os.path.join(directory, category)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for file in os.listdir(class_dir):\n",
    "                    if file.endswith('.avi'):\n",
    "                        self.samples.append((os.path.join(class_dir, file), category))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, category = self.samples[idx]\n",
    "        frames = self.extract_frames(file_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            # Apply transformations to each frame\n",
    "            frames = np.stack([self.transform(frame) for frame in frames])\n",
    "        \n",
    "        label = self.encode_label(category)\n",
    "        return frames, label\n",
    "\n",
    "    def extract_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n",
    "        cap.release()\n",
    "        frames_processed = np.array(frames)\n",
    "        return frames_processed\n",
    "\n",
    "    def encode_label(self, category):\n",
    "        return self.label_encoding[category]\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MicroExpressionDataset(directory='train', transform=transform)\n",
    "test_dataset = MicroExpressionDataset(directory='test', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 3D CNN model architecture\n",
    "class MicroExpressionCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MicroExpressionCNN, self).__init__()\n",
    "        # Example layers (you'll need to adjust in_channels, out_channels, kernel_size, etc.)\n",
    "        self.conv1 = nn.Conv3d(in_channels=3, out_channels=64, kernel_size=(3, 3, 3), stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=2)\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=1, padding=1)\n",
    "        # Add additional layers as needed...\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4 * 4, 512)  # Adjust the dimensions to match the output after flattening\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Add additional layers as needed...\n",
    "        x = x.view(-1, 128 * 4 * 4 * 4)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FedAvg algorithm function\n",
    "def federated_averaging(global_model, client_models):\n",
    "    global_state_dict = global_model.state_dict()\n",
    "    averaged_state_dict = {key: torch.zeros_like(value) for key, value in global_state_dict.items()}\n",
    "    \n",
    "    # Aggregate the parameters from all models\n",
    "    for model in client_models:\n",
    "        local_state_dict = model.state_dict()\n",
    "        for key, value in local_state_dict.items():\n",
    "            averaged_state_dict[key] += value / len(client_models)\n",
    "    \n",
    "    # Load the averaged parameters back into the global model\n",
    "    global_model.load_state_dict(averaged_state_dict)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the global model\n",
    "# Initialize the global model\n",
    "num_classes = 3\n",
    "global_model = MicroExpressionCNN(num_classes=num_classes)\n",
    "\n",
    "# Assume we have defined num_rounds and num_clients\n",
    "num_rounds = 1  # For example, we might want to train for 10 communication rounds\n",
    "num_clients = 1  # Assuming we have 5 clients\n",
    "num_local_epochs = 3\n",
    "\n",
    "# Assume we have a list of dataloaders for each client\n",
    "client_dataloaders = [...]  # A list of PyTorch DataLoader instances for each client\n",
    "\n",
    "# Training loop\n",
    "for round in range(num_rounds):\n",
    "    print(f\"Starting round {round+1}/{num_rounds}\")\n",
    "    client_models = [copy.deepcopy(global_model) for _ in range(num_clients)]\n",
    "    \n",
    "    for client_model, train_loader in zip(client_models, client_dataloaders):\n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=0.01, momentum=0.9)  # Define the optimizer\n",
    "        criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
    "        \n",
    "        client_model.train()  # Set model to training mode\n",
    "        train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "        for epoch in range(num_local_epochs):\n",
    "            for data, target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = client_model(data)  # Forward pass\n",
    "                loss = criterion(output, target)  # Compute the loss\n",
    "                loss.backward()  # Backward pass\n",
    "                optimizer.step()  # Update parameters\n",
    "                print(f\"Client model loss: {loss.item()}\")\n",
    "                pass\n",
    "    \n",
    "    # Update the global model using a federated average of the client models\n",
    "    global_model = federated_averaging(global_model, client_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
